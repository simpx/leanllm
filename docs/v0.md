LeanLLM v0
===========

目标
----
- 150 行内跑通最基础的单请求生成。
- 只依赖 PyTorch + Transformers（Transformers 仅用来拉 config/tokenizer/权重）。

实现
----
- 下载：`AutoConfig` / `AutoTokenizer` / `AutoModelForCausalLM.from_pretrained(..., torch_dtype=torch.float16, device_map="cpu", low_cpu_mem_usage=True)` 获取 config、tokenizer、state_dict。
- 模型：`leanllm/model.py` 纯 PyTorch decoder-only（Embedding -> L×Block -> Final Norm -> LM Head）；Block 含 PreNorm、Rotary MHA（causal mask、可用 `scaled_dot_product_attention`）、MLP+SiLU/GELU；映射 HF 权重名到本地参数名。
- 推理：`leanllm/generate.py` 做 greedy decode，单 batch，无 KV 复用；`device = cuda if available else cpu`；`max_new_tokens` 控制显存占用。
- 串联：`main.py` 调用下载 -> 加载权重 -> forward/generate，打印文本（无并发、无流式）。

用法
----
- 安装：`uv pip install -e .`
- 运行：`uv run main.py --model gpt2 --prompt "Hello" --max-new-tokens 20`。
- 提示：无 GPU 时请用小模型，避免 OOM；仅支持 greedy。
